{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "billsum gen gpt2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIbjW9SY1Ola"
      },
      "source": [
        "# Mathew varghese - NLP A3- billsum\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsAFl5sa80zz",
        "outputId": "dfe2b677-b0d3-4ee9-e1be-4d3683c83e1b"
      },
      "source": [
        "!pip3 install transformers==4.2.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==4.2.2 in /usr/local/lib/python3.6/dist-packages (4.2.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.2) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.2) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.2) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.2) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.2) (0.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.2) (3.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.2) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.2) (0.9.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.2) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.2) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.2.2) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.2.2) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.2.2) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==4.2.2) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.2.2) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.2.2) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.2.2) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.2.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.2.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.2.2) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYpjYetT9Okq",
        "outputId": "b42665ba-dae7-4a00-e506-0ea60450cc16"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/language-modeling/run_clm.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-10 20:20:25--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/language-modeling/run_clm.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17809 (17K) [text/plain]\n",
            "Saving to: ‘run_clm.py.4’\n",
            "\n",
            "run_clm.py.4        100%[===================>]  17.39K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-02-10 20:20:26 (39.9 MB/s) - ‘run_clm.py.4’ saved [17809/17809]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6_zydxkG_Mj",
        "outputId": "0b541476-16f3-464a-c669-8e96484cefc8"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'transformers' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmNmyn1-nN9f",
        "outputId": "6bc4df31-e00a-4e98-ce4b-207f144ffc2c"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Feb 10 20:20:26 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u284DrgOHJl8",
        "outputId": "e4e6c544-d5fb-4d8c-c984-aa14dea94ce2"
      },
      "source": [
        "%%bash\r\n",
        "cd transformers\r\n",
        "pip install ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing /content/transformers\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "    Preparing wheel metadata: started\n",
            "    Preparing wheel metadata: finished with status 'done'\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (0.0.43)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Using cached https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.4.0.dev0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.4.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.4.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.4.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.4.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.4.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.4.0.dev0) (1.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.4.0.dev0) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.4.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==4.4.0.dev0) (2.4.7)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517): started\n",
            "  Building wheel for transformers (PEP 517): finished with status 'done'\n",
            "  Created wheel for transformers: filename=transformers-4.4.0.dev0-cp36-none-any.whl size=1808091 sha256=a31b3ee2339236833b656375c69cedd7ca991013a479d1ac00024f7122cf9dc5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ijb4xp5q/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Found existing installation: tokenizers 0.9.4\n",
            "    Uninstalling tokenizers-0.9.4:\n",
            "      Successfully uninstalled tokenizers-0.9.4\n",
            "  Found existing installation: transformers 4.2.2\n",
            "    Uninstalling transformers-4.2.2:\n",
            "      Successfully uninstalled transformers-4.2.2\n",
            "Successfully installed tokenizers-0.10.1 transformers-4.4.0.dev0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C16mxyM8-pSa",
        "outputId": "8c73ce3b-a6f6-415c-c496-3b12855c459e"
      },
      "source": [
        "!pip3 install datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.6/dist-packages (1.2.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from datasets) (3.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYJTofYXBK_a",
        "outputId": "920ee9fc-2739-4102-989f-65dc47017805"
      },
      "source": [
        "from datasets import load_dataset\r\n",
        "d = load_dataset('billsum')\r\n",
        "d_train = load_dataset('billsum')['train']\r\n",
        "d_val = load_dataset('billsum')['test']\r\n",
        "d_train[0], d_val[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset billsum (/root/.cache/huggingface/datasets/billsum/default/3.0.0/dfbb38fcc96577d054cdcb8accc369486d1a2281a6a2ce938ad7f0b652f3416e)\n",
            "Using custom data configuration default\n",
            "Reusing dataset billsum (/root/.cache/huggingface/datasets/billsum/default/3.0.0/dfbb38fcc96577d054cdcb8accc369486d1a2281a6a2ce938ad7f0b652f3416e)\n",
            "Using custom data configuration default\n",
            "Reusing dataset billsum (/root/.cache/huggingface/datasets/billsum/default/3.0.0/dfbb38fcc96577d054cdcb8accc369486d1a2281a6a2ce938ad7f0b652f3416e)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'summary': \"Shields a business entity from civil liability relating to any injury or death occurring at a facility of that entity in connection with a use of such facility by a nonprofit organization if: (1) the use occurs outside the scope of business of the business entity; (2) such injury or death occurs during a period that such facility is used by such organization; and (3) the business entity authorized the use of such facility by the organization. \\nMakes this Act inapplicable to an injury or death that results from an act or omission of a business entity that constitutes gross negligence or intentional misconduct, including misconduct that: (1) constitutes a hate crime or a crime of violence or act of international terrorism for which the defendant has been convicted in any court; or (2) involves a sexual offense for which the defendant has been convicted in any court or misconduct for which the defendant has been found to have violated a Federal or State civil rights law. \\nPreempts State laws to the extent that such laws are inconsistent with this Act, except State law that provides additional protection from liability.  Specifies that this Act shall not be construed to supersede any Federal or State health or safety law. \\nMakes this Act inapplicable to any civil action in a State court against a business entity in which all parties are citizens of the State if such State, citing this Act's authority and containing no other provision, enacts a statute declaring the State's election that this Act shall not apply to such action in the State.\",\n",
              "  'text': \"SECTION 1. LIABILITY OF BUSINESS ENTITIES PROVIDING USE OF FACILITIES \\n              TO NONPROFIT ORGANIZATIONS.\\n\\n    (a) Definitions.--In this section:\\n            (1) Business entity.--The term ``business entity'' means a \\n        firm, corporation, association, partnership, consortium, joint \\n        venture, or other form of enterprise.\\n            (2) Facility.--The term ``facility'' means any real \\n        property, including any building, improvement, or appurtenance.\\n            (3) Gross negligence.--The term ``gross negligence'' means \\n        voluntary and conscious conduct by a person with knowledge (at \\n        the time of the conduct) that the conduct is likely to be \\n        harmful to the health or well-being of another person.\\n            (4) Intentional misconduct.--The term ``intentional \\n        misconduct'' means conduct by a person with knowledge (at the \\n        time of the conduct) that the conduct is harmful to the health \\n        or well-being of another person.\\n            (5) Nonprofit organization.--The term ``nonprofit \\n        organization'' means--\\n                    (A) any organization described in section 501(c)(3) \\n                of the Internal Revenue Code of 1986 and exempt from \\n                tax under section 501(a) of such Code; or\\n                    (B) any not-for-profit organization organized and \\n                conducted for public benefit and operated primarily for \\n                charitable, civic, educational, religious, welfare, or \\n                health purposes.\\n            (6) State.--The term ``State'' means each of the several \\n        States, the District of Columbia, the Commonwealth of Puerto \\n        Rico, the Virgin Islands, Guam, American Samoa, the Northern \\n        Mariana Islands, any other territory or possession of the \\n        United States, or any political subdivision of any such State, \\n        territory, or possession.\\n    (b) Limitation on Liability.--\\n            (1) In general.--Subject to subsection (c), a business \\n        entity shall not be subject to civil liability relating to any \\n        injury or death occurring at a facility of the business entity \\n        in connection with a use of such facility by a nonprofit \\n        organization if--\\n                    (A) the use occurs outside of the scope of business \\n                of the business entity;\\n                    (B) such injury or death occurs during a period \\n                that such facility is used by the nonprofit \\n                organization; and\\n                    (C) the business entity authorized the use of such \\n                facility by the nonprofit organization.\\n            (2) Application.--This subsection shall apply--\\n                    (A) with respect to civil liability under Federal \\n                and State law; and\\n                    (B) regardless of whether a nonprofit organization \\n                pays for the use of a facility.\\n    (c) Exception for Liability.--Subsection (b) shall not apply to an \\ninjury or death that results from an act or omission of a business \\nentity that constitutes gross negligence or intentional misconduct, \\nincluding any misconduct that--\\n            (1) constitutes a crime of violence (as that term is \\n        defined in section 16 of title 18, United States Code) or act \\n        of international terrorism (as that term is defined in section \\n        2331 of title 18) for which the defendant has been convicted in \\n        any court;\\n            (2) constitutes a hate crime (as that term is used in the \\n        Hate Crime Statistics Act (28 U.S.C. 534 note));\\n            (3) involves a sexual offense, as defined by applicable \\n        State law, for which the defendant has been convicted in any \\n        court; or\\n            (4) involves misconduct for which the defendant has been \\n        found to have violated a Federal or State civil rights law.\\n    (d) Superseding Provision.--\\n            (1) In general.--Subject to paragraph (2) and subsection \\n        (e), this Act preempts the laws of any State to the extent that \\n        such laws are inconsistent with this Act, except that this Act \\n        shall not preempt any State law that provides additional \\n        protection from liability for a business entity for an injury \\n        or death with respect to which conditions under subparagraphs \\n        (A) through (C) of subsection (b)(1) apply.\\n            (2) Limitation.--Nothing in this Act shall be construed to \\n        supersede any Federal or State health or safety law.\\n    (e) Election of State Regarding Nonapplicability.--This Act shall \\nnot apply to any civil action in a State court against a business \\nentity in which all parties are citizens of the State if such State \\nenacts a statute--\\n            (1) citing the authority of this subsection;\\n            (2) declaring the election of such State that this Act \\n        shall not apply to such civil action in the State; and\\n            (3) containing no other provision.\",\n",
              "  'title': 'A bill to limit the civil liability of business entities providing use of facilities to nonprofit organizations.'},\n",
              " {'summary': \"Amends the Water Resources Development Act of 1999 to: (1) authorize appropriations for FY 1999 through 2009 for implementation of a long-term resource monitoring program with respect to the Upper Mississippi River Environmental Management Program (currently, such funding is designated for a program for the planning, construction, and evaluation of measures for fish and wildlife habitat rehabilitation and enhancement); (2) authorize the Secretary of the Army to carry out modifications to the navigation project for the Delaware River, Pennsylvania and Delaware, if such project as modified is technically sound, environmentally (currently, economically) acceptable, and economically justified; (3) subject certain previously deauthorized water resources development projects to the seven-year limitation governing project deauthorizations under the Act, with the exception of such a project for Indian River County, Florida; (4) except from a certain schedule of the non-Federal cost of the periodic nourishment of shore protection projects constructed after December 31, 1999, those projects for which a District Engineer's Report has been completed by such date;  (5) require that the project cooperation agreement for the Comite River Diversion Project for flood control include a provision that specifies that any reduction in the non-Federal share that results from certain modifications be credited toward the share of project costs to be paid by the Amite River Basin Drainage and Water Conservation District; (6) allow the Secretary to provide additional compensation to Chesapeake City, Maryland (currently, to the City of Chesapeake, Maryland) for damage to its water supply resulting from the Chesapeake and Delaware Canal Project; (7) provide for the submission of certain reports on water resources development projects by the Secretary, notwithstanding Federal reporting termination provisions; and (8) authorize and provide for an authorization of appropriations for the existing program for the safety and operations expenses of the Federal Railroad Administration, and make available for obligation funds currently appropriated for such program.\",\n",
              "  'text': \"SECTION 1. ENVIRONMENTAL INFRASTRUCTURE.\\n\\n    (a) Jackson County, Mississippi.--Section 219 of the Water \\nResources Development Act of 1992 (106 Stat. 4835; 110 Stat. 3757) is \\namended--\\n        (1) in subsection (c), by striking paragraph (5) and inserting \\n    the following:\\n        ``(5) Jackson county, mississippi.--Provision of an alternative \\n    water supply and a project for the elimination or control of \\n    combined sewer overflows for Jackson County, Mississippi.''; and\\n        (2) in subsection (e)(1), by striking ``$10,000,000'' and \\n    inserting ``$20,000,000''.\\n    (b) Manchester, New Hampshire.--Section 219(e)(3) of the Water \\nResources Development Act of 1992 (106 Stat. 4835; 110 Stat. 3757) is \\namended by striking ``$10,000,000'' and inserting ``$20,000,000''.\\n    (c) Atlanta, Georgia.--Section 219(f)(1) of the Water Resources \\nDevelopment Act of 1992 (106 Stat. 4835; 113 Stat. 335) is amended by \\nstriking ``$25,000,000 for''.\\n    (d) Paterson, Passaic County, and Passaic Valley, New Jersey.--\\nSection 219(f)(2) of the Water Resources Development Act of 1992 (106 \\nStat. 4835; 113 Stat. 335) is amended by striking ``$20,000,000 for''.\\n    (e) Elizabeth and North Hudson, New Jersey.--Section 219(f) of the \\nWater Resources Development Act of 1992 (106 Stat. 4835; 113 Stat. 335) \\nis amended--\\n        (1) in paragraph (33), by striking ``$20,000,000'' and \\n    inserting ``$10,000,000''; and\\n        (2) in paragraph (34)--\\n            (A) by striking ``$10,000,000'' and inserting \\n        ``$20,000,000''; and\\n            (B) by striking ``in the city of North Hudson'' and \\n        inserting ``for the North Hudson Sewerage Authority''.\\n\\nSEC. 2. UPPER MISSISSIPPI RIVER ENVIRONMENTAL MANAGEMENT PROGRAM.\\n\\n    Section 1103(e)(5) of the Water Resources Development Act of 1986 \\n(33 U.S.C. 652(e)(5)) (as amended by section 509(c)(3) of the Water \\nResources Development Act of 1999 (113 Stat. 340)) is amended by \\nstriking ``paragraph (1)(A)(i)'' and inserting ``paragraph (1)(B)''.\\n\\nSEC. 3. DELAWARE RIVER, PENNSYLVANIA AND DELAWARE.\\n\\n    Section 346 of the Water Resources Development Act of 1999 (113 \\nStat. 309) is amended by striking ``economically acceptable'' and \\ninserting ``environmentally acceptable''.\\n\\nSEC. 4. PROJECT REAUTHORIZATIONS.\\n\\n    Section 364 of the Water Resources Development Act of 1999 (113 \\nStat. 313) is amended--\\n        (1) by striking ``Each'' and all that follows through the colon \\n    and inserting the following: ``Each of the following projects is \\n    authorized to be carried out by the Secretary, and no construction \\n    on any such project may be initiated until the Secretary determines \\n    that the project is technically sound, environmentally acceptable, \\n    and economically justified:'';\\n        (2) by striking paragraph (1); and\\n        (3) by redesignating paragraphs (2) through (6) as paragraphs \\n    (1) through (5), respectively.\\n\\nSEC. 5. SHORE PROTECTION.\\n\\n    Section 103(d)(2)(A) of the Water Resources Development Act of 1986 \\n(33 U.S.C. 2213(d)(2)(A)) (as amended by section 215(a)(2) of the Water \\nResources Development Act of 1999 (113 Stat. 292)) is amended by \\nstriking ``or for which a feasibility study is completed after that \\ndate,'' and inserting ``except for a project for which a District \\nEngineer's Report is completed by that date,''.\\n\\nSEC. 6. COMITE RIVER, LOUISIANA.\\n\\n    Section 371 of the Water Resources Development Act of 1999 (113 \\nStat. 321) is amended--\\n        (1) by inserting ``(a) In General.--'' before ``The''; and\\n        (2) by adding at the end the following:\\n    ``(b) Crediting of Reduction in Non-Federal Share.--The project \\ncooperation agreement for the Comite River Diversion Project shall \\ninclude a provision that specifies that any reduction in the non-\\nFederal share that results from the modification under subsection (a) \\nshall be credited toward the share of project costs to be paid by the \\nAmite River Basin Drainage and Water Conservation District.''.\\n\\nSEC. 7. CHESAPEAKE CITY, MARYLAND.\\n\\n    Section 535(b) of the Water Resources Development Act of 1999 (113 \\nStat. 349) is amended by striking ``the city of Chesapeake'' each place \\nit appears and inserting ``Chesapeake City''.\\n\\nSEC. 8. CONTINUATION OF SUBMISSION OF CERTAIN REPORTS BY THE SECRETARY \\n              OF THE ARMY.\\n\\n    (a) Recommendations of Inland Waterways Users Board.--Section \\n302(b) of the Water Resources Development Act of 1986 (33 U.S.C. \\n2251(b)) is amended in the last sentence by striking ``The'' and \\ninserting ``Notwithstanding section 3003 of Public Law 104-66 (31 \\nU.S.C. 1113 note; 109 Stat. 734), the''.\\n    (b) List of Authorized but Unfunded Studies.--Section 710(a) of the \\nWater Resources Development Act of 1986 (33 U.S.C. 2264(a)) is amended \\nin the first sentence by striking ``Not'' and inserting \\n``Notwithstanding section 3003 of Public Law 104-66 (31 U.S.C. 1113 \\nnote; 109 Stat. 734), not''.\\n    (c) Reports on Participation of Minority Groups and Minority-Owned \\nFirms in Mississippi River-Gulf Outlet Feature.--Section 844(b) of the \\nWater Resources Development Act of 1986 (100 Stat. 4177) is amended in \\nthe second sentence by striking ``The'' and inserting ``Notwithstanding \\nsection 3003 of Public Law 104-66 (31 U.S.C. 1113 note; 109 Stat. 734), \\nthe''.\\n    (d) List of Authorized but Unfunded Projects.--Section 1001(b)(2) \\nof the Water Resources Development Act of 1986 (33 U.S.C. 579a(b)(2)) \\nis amended in the first sentence by striking ``Every'' and inserting \\n``Notwithstanding section 3003 of Public Law 104-66 (31 U.S.C. 1113 \\nnote; 109 Stat. 734), every''.\\n\\nSEC. 9. AUTHORIZATIONS FOR PROGRAM PREVIOUSLY AND CURRENTLY FUNDED.\\n\\n    (a) Program Authorization.--The program described in subsection (c) \\nis hereby authorized.\\n    (b) Authorization of Appropriations.--Funds are hereby authorized \\nto be appropriated for the Department of Transportation for the program \\nauthorized in subsection (a) in amounts as follows:\\n        (1) Fiscal year 2000.--For fiscal year 2000, $10,000,000.\\n        (2) Fiscal year 2001.--For fiscal year 2001, $10,000,000.\\n        (3) Fiscal year 2002.--For fiscal year 2002, $7,000,000.\\n    (c) Applicability.--The program referred to in subsection (a) is \\nthe program for which funds appropriated in title I of Public Law 106-\\n69 under the heading ``FEDERAL RAILROAD ADMINISTRATION'' are available \\nfor obligation upon the enactment of legislation authorizing the \\nprogram.\\n\\n                               Speaker of the House of Representatives.\\n\\n                            Vice President of the United States and    \\n                                               President of the Senate.\",\n",
              "  'title': 'To make technical corrections to the Water Resources Development Act of 1999.'})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9pDa1shnmA_",
        "outputId": "2f9737c3-1203-4309-8ee2-e9012fca1b68"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQciWLclBpv1"
      },
      "source": [
        "f = open(\"/content/drive/MyDrive/NLP/billsum/d_train.txt\", \"a\")\r\n",
        "f.write(d_train[0]['text'])\r\n",
        "f.close()\r\n",
        "f = open(\"/content/drive/MyDrive/NLP/billsum/d_val.txt\", \"a\")\r\n",
        "f.write(d_val[0]['text'])\r\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZ8QAxOfGepp",
        "outputId": "68b5d5df-49b0-4163-94b8-f1e6edec5f85"
      },
      "source": [
        "%%bash\r\n",
        "python run_clm.py \\\r\n",
        "--model_type gpt2-medium \\\r\n",
        "--model_name_or_path gpt2-medium \\\r\n",
        "--train_file /content/drive/MyDrive/NLP/billsum/d_train.txt \\\r\n",
        "--do_train \\\r\n",
        "--validation_file /content/drive/MyDrive/NLP/billsum/d_val.txt \\\r\n",
        "--do_eval \\\r\n",
        "--per_gpu_train_batch_size 1 \\\r\n",
        "--save_steps -1 \\\r\n",
        "--num_train_epochs 1 \\\r\n",
        "--fp16 \\\r\n",
        "--learning_rate 5e-5 \\\r\n",
        "--output_dir /content/drive/MyDrive/NLP/billsum/output1 \\\r\n",
        "--overwrite_output_dir \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/10/2021 20:20:47 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "02/10/2021 20:20:47 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/content/drive/MyDrive/NLP/billsum/output1, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Feb10_20-20-47_38bb79fdf945, logging_first_step=False, logging_steps=500, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/content/drive/MyDrive/NLP/billsum/output1, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, _n_gpu=1)\n",
            "Downloading and preparing dataset text/default-578db1fdc85307a7 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-578db1fdc85307a7/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-578db1fdc85307a7/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab. Subsequent calls will reuse this data.\n",
            "02/10/2021 20:20:48 - INFO - transformers.configuration_utils -   loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "02/10/2021 20:20:48 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/10/2021 20:20:49 - INFO - transformers.configuration_utils -   loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "02/10/2021 20:20:49 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/10/2021 20:20:49 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "02/10/2021 20:20:49 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "02/10/2021 20:20:49 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "02/10/2021 20:20:49 - INFO - transformers.modeling_utils -   loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
            "02/10/2021 20:21:44 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "02/10/2021 20:21:44 - INFO - transformers.modeling_utils -   All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "02/10/2021 20:21:49 - INFO - transformers.trainer -   The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "02/10/2021 20:21:49 - INFO - transformers.trainer -   The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "02/10/2021 20:21:49 - INFO - transformers.trainer -   Using amp fp16 backend\n",
            "02/10/2021 20:21:49 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "02/10/2021 20:21:49 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "02/10/2021 20:21:49 - INFO - transformers.trainer -   ***** Running training *****\n",
            "02/10/2021 20:21:49 - INFO - transformers.trainer -     Num examples = 6\n",
            "02/10/2021 20:21:49 - INFO - transformers.trainer -     Num Epochs = 1\n",
            "02/10/2021 20:21:49 - INFO - transformers.trainer -     Instantaneous batch size per device = 8\n",
            "02/10/2021 20:21:49 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "02/10/2021 20:21:49 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
            "02/10/2021 20:21:49 - INFO - transformers.trainer -     Total optimization steps = 6\n",
            "02/10/2021 20:21:49 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "02/10/2021 20:21:53 - INFO - transformers.trainer -   \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 3.8624, 'train_samples_per_second': 1.553, 'epoch': 1.0}\n",
            "02/10/2021 20:21:53 - INFO - transformers.trainer -   Saving model checkpoint to /content/drive/MyDrive/NLP/billsum/output1\n",
            "02/10/2021 20:21:53 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/MyDrive/NLP/billsum/output1/config.json\n",
            "02/10/2021 20:22:03 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/MyDrive/NLP/billsum/output1/pytorch_model.bin\n",
            "02/10/2021 20:22:03 - INFO - __main__ -   ***** Train results *****\n",
            "02/10/2021 20:22:03 - INFO - __main__ -     epoch = 1.0\n",
            "02/10/2021 20:22:03 - INFO - __main__ -     train_runtime = 3.8624\n",
            "02/10/2021 20:22:03 - INFO - __main__ -     train_samples_per_second = 1.553\n",
            "02/10/2021 20:22:03 - INFO - __main__ -   *** Evaluate ***\n",
            "02/10/2021 20:22:03 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
            "02/10/2021 20:22:03 - INFO - transformers.trainer -     Num examples = 7\n",
            "02/10/2021 20:22:03 - INFO - transformers.trainer -     Batch size = 8\n",
            "02/10/2021 20:22:05 - INFO - __main__ -   ***** Eval results *****\n",
            "02/10/2021 20:22:05 - INFO - __main__ -     eval_loss = 1.6328084468841553\n",
            "02/10/2021 20:22:05 - INFO - __main__ -     perplexity = 5.118228826527366\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-02-10 20:20:46.514455: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "Using custom data configuration default\n",
            "\r0 tables [00:00, ? tables/s]\r                            \r\r0 tables [00:00, ? tables/s]\r                            \r[INFO|configuration_utils.py:456] 2021-02-10 20:20:48,845 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:492] 2021-02-10 20:20:48,845 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:456] 2021-02-10 20:20:49,057 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:492] 2021-02-10 20:20:49,058 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 20:20:49,714 >> loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 20:20:49,714 >> loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 20:20:49,714 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|modeling_utils.py:1027] 2021-02-10 20:20:49,996 >> loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
            "[INFO|modeling_utils.py:1143] 2021-02-10 20:21:44,761 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1152] 2021-02-10 20:21:44,761 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "\r  0%|          | 0/1 [00:00<?, ?ba/s]\r100%|██████████| 1/1 [00:00<00:00,  2.39ba/s]\r100%|██████████| 1/1 [00:00<00:00,  2.39ba/s]\n",
            "\r  0%|          | 0/1 [00:00<?, ?ba/s]\r100%|██████████| 1/1 [00:00<00:00, 57.65ba/s]\n",
            "\r  0%|          | 0/1 [00:00<?, ?ba/s]\r100%|██████████| 1/1 [00:00<00:00, 68.61ba/s]\n",
            "\r  0%|          | 0/1 [00:00<?, ?ba/s]\r100%|██████████| 1/1 [00:00<00:00, 44.78ba/s]\n",
            "[INFO|trainer.py:434] 2021-02-10 20:21:49,791 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:434] 2021-02-10 20:21:49,791 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:350] 2021-02-10 20:21:49,792 >> Using amp fp16 backend\n",
            "[WARNING|training_args.py:514] 2021-02-10 20:21:49,792 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:514] 2021-02-10 20:21:49,798 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:839] 2021-02-10 20:21:49,798 >> ***** Running training *****\n",
            "[INFO|trainer.py:840] 2021-02-10 20:21:49,798 >>   Num examples = 6\n",
            "[INFO|trainer.py:841] 2021-02-10 20:21:49,798 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:842] 2021-02-10 20:21:49,798 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:843] 2021-02-10 20:21:49,798 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:844] 2021-02-10 20:21:49,798 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:845] 2021-02-10 20:21:49,798 >>   Total optimization steps = 6\n",
            "[WARNING|training_args.py:514] 2021-02-10 20:21:49,809 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "\r  0%|          | 0/6 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "\r 17%|█▋        | 1/6 [00:00<00:03,  1.57it/s]\r 33%|███▎      | 2/6 [00:01<00:02,  1.59it/s]\r 50%|█████     | 3/6 [00:01<00:01,  1.56it/s]\r 67%|██████▋   | 4/6 [00:02<00:01,  1.55it/s]\r 83%|████████▎ | 5/6 [00:03<00:00,  1.57it/s]\r100%|██████████| 6/6 [00:03<00:00,  1.56it/s][INFO|trainer.py:1013] 2021-02-10 20:21:53,661 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "\r                                             \r\r100%|██████████| 6/6 [00:03<00:00,  1.56it/s]\r100%|██████████| 6/6 [00:03<00:00,  1.56it/s]\n",
            "[INFO|trainer.py:1422] 2021-02-10 20:21:53,663 >> Saving model checkpoint to /content/drive/MyDrive/NLP/billsum/output1\n",
            "[INFO|configuration_utils.py:311] 2021-02-10 20:21:53,669 >> Configuration saved in /content/drive/MyDrive/NLP/billsum/output1/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-02-10 20:22:03,350 >> Model weights saved in /content/drive/MyDrive/NLP/billsum/output1/pytorch_model.bin\n",
            "[INFO|trainer.py:1620] 2021-02-10 20:22:03,474 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1621] 2021-02-10 20:22:03,474 >>   Num examples = 7\n",
            "[INFO|trainer.py:1622] 2021-02-10 20:22:03,475 >>   Batch size = 8\n",
            "\r  0%|          | 0/1 [00:00<?, ?it/s]\r100%|██████████| 1/1 [00:00<00:00,  9.68it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeDH8wsBo1Lm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f402ad78-cb3e-441b-f2ba-37b6e09c9161"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/      run_clm.py.1  run_clm.py.3  \u001b[01;34mruns\u001b[0m/         \u001b[01;34msession\u001b[0m/\n",
            "run_clm.py  run_clm.py.2  run_clm.py.4  \u001b[01;34msample_data\u001b[0m/  \u001b[01;34mtransformers\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX-1SwWp95s9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e1e7045-1524-4181-a05c-0751ae66e77f"
      },
      "source": [
        "%%bash\r\n",
        "python run_clm.py \\\r\n",
        "--model_type gpt2 \\\r\n",
        "--model_name_or_path gpt2 \\\r\n",
        "--train_file /content/drive/MyDrive/NLP/billsum/d_train.txt \\\r\n",
        "--do_train \\\r\n",
        "--validation_file /content/drive/MyDrive/NLP/billsum/d_val.txt \\\r\n",
        "--do_eval \\\r\n",
        "--per_gpu_train_batch_size 1 \\\r\n",
        "--save_steps -1 \\\r\n",
        "--num_train_epochs 3 \\\r\n",
        "--fp16 \\\r\n",
        "--learning_rate 5e-5 \\\r\n",
        "--output_dir /content/drive/MyDrive/NLP/billsum/output3 \\\r\n",
        "--overwrite_output_dir \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/10/2021 20:22:12 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "02/10/2021 20:22:12 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/content/drive/MyDrive/NLP/billsum/output3, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Feb10_20-22-12_38bb79fdf945, logging_first_step=False, logging_steps=500, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/content/drive/MyDrive/NLP/billsum/output3, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, _n_gpu=1)\n",
            "02/10/2021 20:22:12 - INFO - transformers.configuration_utils -   loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "02/10/2021 20:22:12 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/10/2021 20:22:13 - INFO - transformers.configuration_utils -   loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "02/10/2021 20:22:13 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/10/2021 20:22:14 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "02/10/2021 20:22:14 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "02/10/2021 20:22:14 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "02/10/2021 20:22:14 - INFO - transformers.modeling_utils -   loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "02/10/2021 20:22:34 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "02/10/2021 20:22:34 - INFO - transformers.modeling_utils -   All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "02/10/2021 20:22:39 - INFO - transformers.trainer -   The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "02/10/2021 20:22:39 - INFO - transformers.trainer -   The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "02/10/2021 20:22:39 - INFO - transformers.trainer -   Using amp fp16 backend\n",
            "02/10/2021 20:22:39 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "02/10/2021 20:22:39 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "02/10/2021 20:22:39 - INFO - transformers.trainer -   ***** Running training *****\n",
            "02/10/2021 20:22:39 - INFO - transformers.trainer -     Num examples = 6\n",
            "02/10/2021 20:22:39 - INFO - transformers.trainer -     Num Epochs = 3\n",
            "02/10/2021 20:22:39 - INFO - transformers.trainer -     Instantaneous batch size per device = 8\n",
            "02/10/2021 20:22:39 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "02/10/2021 20:22:39 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
            "02/10/2021 20:22:39 - INFO - transformers.trainer -     Total optimization steps = 18\n",
            "02/10/2021 20:22:39 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "02/10/2021 20:22:43 - INFO - transformers.trainer -   \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 4.3569, 'train_samples_per_second': 4.131, 'epoch': 3.0}\n",
            "02/10/2021 20:22:43 - INFO - transformers.trainer -   Saving model checkpoint to /content/drive/MyDrive/NLP/billsum/output3\n",
            "02/10/2021 20:22:43 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/MyDrive/NLP/billsum/output3/config.json\n",
            "02/10/2021 20:22:46 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/MyDrive/NLP/billsum/output3/pytorch_model.bin\n",
            "02/10/2021 20:22:46 - INFO - __main__ -   ***** Train results *****\n",
            "02/10/2021 20:22:46 - INFO - __main__ -     epoch = 3.0\n",
            "02/10/2021 20:22:46 - INFO - __main__ -     train_runtime = 4.3569\n",
            "02/10/2021 20:22:46 - INFO - __main__ -     train_samples_per_second = 4.131\n",
            "02/10/2021 20:22:46 - INFO - __main__ -   *** Evaluate ***\n",
            "02/10/2021 20:22:46 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
            "02/10/2021 20:22:46 - INFO - transformers.trainer -     Num examples = 7\n",
            "02/10/2021 20:22:46 - INFO - transformers.trainer -     Batch size = 8\n",
            "02/10/2021 20:22:46 - INFO - __main__ -   ***** Eval results *****\n",
            "02/10/2021 20:22:46 - INFO - __main__ -     eval_loss = 1.7325612306594849\n",
            "02/10/2021 20:22:46 - INFO - __main__ -     perplexity = 5.655119441033231\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-02-10 20:22:09.324526: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "Using custom data configuration default\n",
            "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-578db1fdc85307a7/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab)\n",
            "[INFO|configuration_utils.py:456] 2021-02-10 20:22:12,984 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:492] 2021-02-10 20:22:12,984 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:456] 2021-02-10 20:22:13,206 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:492] 2021-02-10 20:22:13,207 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 20:22:14,028 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 20:22:14,028 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 20:22:14,028 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|modeling_utils.py:1027] 2021-02-10 20:22:14,325 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1143] 2021-02-10 20:22:34,963 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1152] 2021-02-10 20:22:34,963 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "\r  0%|          | 0/1 [00:00<?, ?ba/s]\r100%|██████████| 1/1 [00:00<00:00, 32.02ba/s]\n",
            "\r  0%|          | 0/1 [00:00<?, ?ba/s]\r100%|██████████| 1/1 [00:00<00:00, 57.79ba/s]\n",
            "\r  0%|          | 0/1 [00:00<?, ?ba/s]\r100%|██████████| 1/1 [00:00<00:00, 70.21ba/s]\n",
            "\r  0%|          | 0/1 [00:00<?, ?ba/s]\r100%|██████████| 1/1 [00:00<00:00, 45.30ba/s]\n",
            "[INFO|trainer.py:434] 2021-02-10 20:22:39,496 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:434] 2021-02-10 20:22:39,497 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:350] 2021-02-10 20:22:39,497 >> Using amp fp16 backend\n",
            "[WARNING|training_args.py:514] 2021-02-10 20:22:39,498 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:514] 2021-02-10 20:22:39,501 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:839] 2021-02-10 20:22:39,501 >> ***** Running training *****\n",
            "[INFO|trainer.py:840] 2021-02-10 20:22:39,501 >>   Num examples = 6\n",
            "[INFO|trainer.py:841] 2021-02-10 20:22:39,501 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:842] 2021-02-10 20:22:39,501 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:843] 2021-02-10 20:22:39,501 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:844] 2021-02-10 20:22:39,501 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:845] 2021-02-10 20:22:39,501 >>   Total optimization steps = 18\n",
            "[WARNING|training_args.py:514] 2021-02-10 20:22:39,506 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "\r  0%|          | 0/18 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "\r  6%|▌         | 1/18 [00:00<00:03,  4.30it/s]\r 11%|█         | 2/18 [00:00<00:03,  4.31it/s]\r 17%|█▋        | 3/18 [00:00<00:03,  4.22it/s]\r 22%|██▏       | 4/18 [00:00<00:03,  4.25it/s]\r 28%|██▊       | 5/18 [00:01<00:03,  4.28it/s]\r 33%|███▎      | 6/18 [00:01<00:02,  4.30it/s]\r 39%|███▉      | 7/18 [00:01<00:02,  4.24it/s]\r 44%|████▍     | 8/18 [00:01<00:02,  4.19it/s]\r 50%|█████     | 9/18 [00:02<00:02,  4.16it/s]\r 56%|█████▌    | 10/18 [00:02<00:01,  4.14it/s]\r 61%|██████    | 11/18 [00:02<00:01,  4.08it/s]\r 67%|██████▋   | 12/18 [00:02<00:01,  4.09it/s]\r 72%|███████▏  | 13/18 [00:03<00:01,  4.09it/s]\r 78%|███████▊  | 14/18 [00:03<00:00,  4.10it/s]\r 83%|████████▎ | 15/18 [00:03<00:00,  4.09it/s]\r 89%|████████▉ | 16/18 [00:03<00:00,  4.09it/s]\r 94%|█████████▍| 17/18 [00:04<00:00,  4.09it/s]\r100%|██████████| 18/18 [00:04<00:00,  4.07it/s][INFO|trainer.py:1013] 2021-02-10 20:22:43,858 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "\r                                               \r\r100%|██████████| 18/18 [00:04<00:00,  4.07it/s]\r100%|██████████| 18/18 [00:04<00:00,  4.14it/s]\n",
            "[INFO|trainer.py:1422] 2021-02-10 20:22:43,861 >> Saving model checkpoint to /content/drive/MyDrive/NLP/billsum/output3\n",
            "[INFO|configuration_utils.py:311] 2021-02-10 20:22:43,867 >> Configuration saved in /content/drive/MyDrive/NLP/billsum/output3/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-02-10 20:22:46,394 >> Model weights saved in /content/drive/MyDrive/NLP/billsum/output3/pytorch_model.bin\n",
            "[INFO|trainer.py:1620] 2021-02-10 20:22:46,515 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1621] 2021-02-10 20:22:46,515 >>   Num examples = 7\n",
            "[INFO|trainer.py:1622] 2021-02-10 20:22:46,516 >>   Batch size = 8\n",
            "\r  0%|          | 0/1 [00:00<?, ?it/s]\r100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQPN-rcfMK0x",
        "outputId": "2f388fca-b705-4c26-9c42-7d32b6566d8b"
      },
      "source": [
        "%%bash\r\n",
        "python run_clm.py \\\r\n",
        "--model_type gpt2 \\\r\n",
        "--model_name_or_path gpt2 \\\r\n",
        "--train_file /content/drive/MyDrive/NLP/billsum/d_train.txt \\\r\n",
        "--do_train \\\r\n",
        "--validation_file /content/drive/MyDrive/NLP/billsum/d_val.txt \\\r\n",
        "--do_eval \\\r\n",
        "--per_gpu_train_batch_size 1 \\\r\n",
        "--save_steps -1 \\\r\n",
        "--num_train_epochs 5 \\\r\n",
        "--fp16 \\\r\n",
        "--learning_rate 5e-5 \\\r\n",
        "--output_dir /content/drive/MyDrive/NLP/billsum/output5 \\\r\n",
        "--overwrite_output_dir \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/10/2021 20:22:51 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "02/10/2021 20:22:51 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/content/drive/MyDrive/NLP/billsum/output5, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Feb10_20-22-51_38bb79fdf945, logging_first_step=False, logging_steps=500, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/content/drive/MyDrive/NLP/billsum/output5, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, _n_gpu=1)\n",
            "02/10/2021 20:22:51 - INFO - transformers.configuration_utils -   loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "02/10/2021 20:22:51 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/10/2021 20:22:52 - INFO - transformers.configuration_utils -   loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "02/10/2021 20:22:52 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/10/2021 20:22:52 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "02/10/2021 20:22:52 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "02/10/2021 20:22:52 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "02/10/2021 20:22:53 - INFO - transformers.modeling_utils -   loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "02/10/2021 20:22:58 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "02/10/2021 20:22:58 - INFO - transformers.modeling_utils -   All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "02/10/2021 20:23:03 - INFO - transformers.trainer -   The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "02/10/2021 20:23:03 - INFO - transformers.trainer -   The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "02/10/2021 20:23:03 - INFO - transformers.trainer -   Using amp fp16 backend\n",
            "02/10/2021 20:23:03 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "02/10/2021 20:23:03 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "02/10/2021 20:23:03 - INFO - transformers.trainer -   ***** Running training *****\n",
            "02/10/2021 20:23:03 - INFO - transformers.trainer -     Num examples = 6\n",
            "02/10/2021 20:23:03 - INFO - transformers.trainer -     Num Epochs = 5\n",
            "02/10/2021 20:23:03 - INFO - transformers.trainer -     Instantaneous batch size per device = 8\n",
            "02/10/2021 20:23:03 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "02/10/2021 20:23:03 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
            "02/10/2021 20:23:03 - INFO - transformers.trainer -     Total optimization steps = 30\n",
            "02/10/2021 20:23:03 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "02/10/2021 20:23:10 - INFO - transformers.trainer -   \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 7.3118, 'train_samples_per_second': 4.103, 'epoch': 5.0}\n",
            "02/10/2021 20:23:10 - INFO - transformers.trainer -   Saving model checkpoint to /content/drive/MyDrive/NLP/billsum/output5\n",
            "02/10/2021 20:23:10 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/MyDrive/NLP/billsum/output5/config.json\n",
            "02/10/2021 20:23:13 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/MyDrive/NLP/billsum/output5/pytorch_model.bin\n",
            "02/10/2021 20:23:13 - INFO - __main__ -   ***** Train results *****\n",
            "02/10/2021 20:23:13 - INFO - __main__ -     epoch = 5.0\n",
            "02/10/2021 20:23:13 - INFO - __main__ -     train_runtime = 7.3118\n",
            "02/10/2021 20:23:13 - INFO - __main__ -     train_samples_per_second = 4.103\n",
            "02/10/2021 20:23:13 - INFO - __main__ -   *** Evaluate ***\n",
            "02/10/2021 20:23:13 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
            "02/10/2021 20:23:13 - INFO - transformers.trainer -     Num examples = 7\n",
            "02/10/2021 20:23:13 - INFO - transformers.trainer -     Batch size = 8\n",
            "02/10/2021 20:23:14 - INFO - __main__ -   ***** Eval results *****\n",
            "02/10/2021 20:23:14 - INFO - __main__ -     eval_loss = 1.7082064151763916\n",
            "02/10/2021 20:23:14 - INFO - __main__ -     perplexity = 5.519053704099601\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-02-10 20:22:49.858493: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "Using custom data configuration default\n",
            "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-578db1fdc85307a7/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab)\n",
            "[INFO|configuration_utils.py:456] 2021-02-10 20:22:51,876 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:492] 2021-02-10 20:22:51,876 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:456] 2021-02-10 20:22:52,219 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:492] 2021-02-10 20:22:52,220 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 20:22:52,862 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 20:22:52,862 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 20:22:52,862 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|modeling_utils.py:1027] 2021-02-10 20:22:53,134 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1143] 2021-02-10 20:22:58,681 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1152] 2021-02-10 20:22:58,681 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-578db1fdc85307a7/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-ecc9266fbb545492.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-578db1fdc85307a7/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-49795fa8b8bd449b.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-578db1fdc85307a7/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-53f0e6cd35cfbccf.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-578db1fdc85307a7/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-6dc34314cfc55680.arrow\n",
            "[INFO|trainer.py:434] 2021-02-10 20:23:03,309 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:434] 2021-02-10 20:23:03,309 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:350] 2021-02-10 20:23:03,310 >> Using amp fp16 backend\n",
            "[WARNING|training_args.py:514] 2021-02-10 20:23:03,310 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:514] 2021-02-10 20:23:03,314 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:839] 2021-02-10 20:23:03,314 >> ***** Running training *****\n",
            "[INFO|trainer.py:840] 2021-02-10 20:23:03,314 >>   Num examples = 6\n",
            "[INFO|trainer.py:841] 2021-02-10 20:23:03,314 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:842] 2021-02-10 20:23:03,314 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:843] 2021-02-10 20:23:03,314 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:844] 2021-02-10 20:23:03,314 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:845] 2021-02-10 20:23:03,314 >>   Total optimization steps = 30\n",
            "[WARNING|training_args.py:514] 2021-02-10 20:23:03,319 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "\r  0%|          | 0/30 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "\r  3%|▎         | 1/30 [00:00<00:06,  4.27it/s]\r  7%|▋         | 2/30 [00:00<00:06,  4.29it/s]\r 10%|█         | 3/30 [00:00<00:06,  4.20it/s]\r 13%|█▎        | 4/30 [00:00<00:06,  4.23it/s]\r 17%|█▋        | 5/30 [00:01<00:05,  4.27it/s]\r 20%|██        | 6/30 [00:01<00:05,  4.29it/s]\r 23%|██▎       | 7/30 [00:01<00:05,  4.22it/s]\r 27%|██▋       | 8/30 [00:01<00:05,  4.19it/s]\r 30%|███       | 9/30 [00:02<00:05,  4.12it/s]\r 33%|███▎      | 10/30 [00:02<00:04,  4.11it/s]\r 37%|███▋      | 11/30 [00:02<00:04,  4.10it/s]\r 40%|████      | 12/30 [00:02<00:04,  4.10it/s]\r 43%|████▎     | 13/30 [00:03<00:04,  4.10it/s]\r 47%|████▋     | 14/30 [00:03<00:03,  4.05it/s]\r 50%|█████     | 15/30 [00:03<00:03,  4.06it/s]\r 53%|█████▎    | 16/30 [00:03<00:03,  4.06it/s]\r 57%|█████▋    | 17/30 [00:04<00:03,  4.07it/s]\r 60%|██████    | 18/30 [00:04<00:02,  4.07it/s]\r 63%|██████▎   | 19/30 [00:04<00:02,  4.08it/s]\r 67%|██████▋   | 20/30 [00:04<00:02,  4.08it/s]\r 70%|███████   | 21/30 [00:05<00:02,  4.08it/s]\r 73%|███████▎  | 22/30 [00:05<00:01,  4.07it/s]\r 77%|███████▋  | 23/30 [00:05<00:01,  4.05it/s]\r 80%|████████  | 24/30 [00:05<00:01,  4.06it/s]\r 83%|████████▎ | 25/30 [00:06<00:01,  4.07it/s]\r 87%|████████▋ | 26/30 [00:06<00:00,  4.08it/s]\r 90%|█████████ | 27/30 [00:06<00:00,  4.08it/s]\r 93%|█████████▎| 28/30 [00:06<00:00,  4.09it/s]\r 97%|█████████▋| 29/30 [00:07<00:00,  4.09it/s]\r100%|██████████| 30/30 [00:07<00:00,  4.08it/s][INFO|trainer.py:1013] 2021-02-10 20:23:10,626 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "\r                                               \r\r100%|██████████| 30/30 [00:07<00:00,  4.08it/s]\r100%|██████████| 30/30 [00:07<00:00,  4.11it/s]\n",
            "[INFO|trainer.py:1422] 2021-02-10 20:23:10,628 >> Saving model checkpoint to /content/drive/MyDrive/NLP/billsum/output5\n",
            "[INFO|configuration_utils.py:311] 2021-02-10 20:23:10,634 >> Configuration saved in /content/drive/MyDrive/NLP/billsum/output5/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-02-10 20:23:13,363 >> Model weights saved in /content/drive/MyDrive/NLP/billsum/output5/pytorch_model.bin\n",
            "[INFO|trainer.py:1620] 2021-02-10 20:23:13,920 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1621] 2021-02-10 20:23:13,920 >>   Num examples = 7\n",
            "[INFO|trainer.py:1622] 2021-02-10 20:23:13,921 >>   Batch size = 8\n",
            "\r  0%|          | 0/1 [00:00<?, ?it/s]\r100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_66-DsPMOhD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41930d12-1253-4ffa-87e7-2d33746a0ffa"
      },
      "source": [
        "%%bash\r\n",
        "python run_clm.py \\\r\n",
        "--model_type gpt2-medium \\\r\n",
        "--model_name_or_path gpt2-medium \\\r\n",
        "--train_file /content/drive/MyDrive/NLP/billsum/d_train.txt \\\r\n",
        "--do_train \\\r\n",
        "--validation_file /content/drive/MyDrive/NLP/billsum/d_val.txt \\\r\n",
        "--do_eval \\\r\n",
        "--per_gpu_train_batch_size 1 \\\r\n",
        "--save_steps -1 \\\r\n",
        "--num_train_epochs 7 \\\r\n",
        "--fp16 \\\r\n",
        "--learning_rate 5e-5 \\\r\n",
        "--output_dir /content/drive/MyDrive/NLP/billsum/output7 \\\r\n",
        "--overwrite_output_dir \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/10/2021 20:23:18 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "02/10/2021 20:23:18 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/content/drive/MyDrive/NLP/billsum/output7, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=7.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Feb10_20-23-18_38bb79fdf945, logging_first_step=False, logging_steps=500, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/content/drive/MyDrive/NLP/billsum/output7, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, _n_gpu=1)\n",
            "02/10/2021 20:23:19 - INFO - transformers.configuration_utils -   loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "02/10/2021 20:23:19 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/10/2021 20:23:19 - INFO - transformers.configuration_utils -   loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "02/10/2021 20:23:19 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/10/2021 20:23:20 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "02/10/2021 20:23:20 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "02/10/2021 20:23:20 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "02/10/2021 20:23:20 - INFO - transformers.modeling_utils -   loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
            "02/10/2021 20:24:15 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "02/10/2021 20:24:15 - INFO - transformers.modeling_utils -   All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "02/10/2021 20:24:20 - INFO - transformers.trainer -   The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "02/10/2021 20:24:20 - INFO - transformers.trainer -   The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "02/10/2021 20:24:20 - INFO - transformers.trainer -   Using amp fp16 backend\n",
            "02/10/2021 20:24:20 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "02/10/2021 20:24:20 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "02/10/2021 20:24:20 - INFO - transformers.trainer -   ***** Running training *****\n",
            "02/10/2021 20:24:20 - INFO - transformers.trainer -     Num examples = 6\n",
            "02/10/2021 20:24:20 - INFO - transformers.trainer -     Num Epochs = 7\n",
            "02/10/2021 20:24:20 - INFO - transformers.trainer -     Instantaneous batch size per device = 8\n",
            "02/10/2021 20:24:20 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "02/10/2021 20:24:20 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
            "02/10/2021 20:24:20 - INFO - transformers.trainer -     Total optimization steps = 42\n",
            "02/10/2021 20:24:20 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "02/10/2021 20:24:48 - INFO - transformers.trainer -   \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 27.5695, 'train_samples_per_second': 1.523, 'epoch': 7.0}\n",
            "02/10/2021 20:24:48 - INFO - transformers.trainer -   Saving model checkpoint to /content/drive/MyDrive/NLP/billsum/output7\n",
            "02/10/2021 20:24:48 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/MyDrive/NLP/billsum/output7/config.json\n",
            "02/10/2021 20:24:56 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/MyDrive/NLP/billsum/output7/pytorch_model.bin\n",
            "02/10/2021 20:24:58 - INFO - __main__ -   ***** Train results *****\n",
            "02/10/2021 20:24:58 - INFO - __main__ -     epoch = 7.0\n",
            "02/10/2021 20:24:58 - INFO - __main__ -     train_runtime = 27.5695\n",
            "02/10/2021 20:24:58 - INFO - __main__ -     train_samples_per_second = 1.523\n",
            "02/10/2021 20:24:58 - INFO - __main__ -   *** Evaluate ***\n",
            "02/10/2021 20:24:58 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
            "02/10/2021 20:24:58 - INFO - transformers.trainer -     Num examples = 7\n",
            "02/10/2021 20:24:58 - INFO - transformers.trainer -     Batch size = 8\n",
            "02/10/2021 20:25:00 - INFO - __main__ -   ***** Eval results *****\n",
            "02/10/2021 20:25:00 - INFO - __main__ -     eval_loss = 1.6993114948272705\n",
            "02/10/2021 20:25:00 - INFO - __main__ -     perplexity = 5.470179847768329\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-02-10 20:23:17.247153: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "Using custom data configuration default\n",
            "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-578db1fdc85307a7/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab)\n",
            "[INFO|configuration_utils.py:456] 2021-02-10 20:23:19,491 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:492] 2021-02-10 20:23:19,491 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:456] 2021-02-10 20:23:19,696 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:492] 2021-02-10 20:23:19,697 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 20:23:20,352 >> loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 20:23:20,352 >> loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 20:23:20,352 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|modeling_utils.py:1027] 2021-02-10 20:23:20,633 >> loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
            "[INFO|modeling_utils.py:1143] 2021-02-10 20:24:15,990 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1152] 2021-02-10 20:24:15,991 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-578db1fdc85307a7/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-f617af501a9bc72b.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-578db1fdc85307a7/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-3ba40ac3463a7942.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-578db1fdc85307a7/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-e3eab56b40b1d795.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-578db1fdc85307a7/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-c55194e233704f3f.arrow\n",
            "[INFO|trainer.py:434] 2021-02-10 20:24:20,973 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:434] 2021-02-10 20:24:20,973 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:350] 2021-02-10 20:24:20,974 >> Using amp fp16 backend\n",
            "[WARNING|training_args.py:514] 2021-02-10 20:24:20,974 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:514] 2021-02-10 20:24:20,980 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:839] 2021-02-10 20:24:20,980 >> ***** Running training *****\n",
            "[INFO|trainer.py:840] 2021-02-10 20:24:20,980 >>   Num examples = 6\n",
            "[INFO|trainer.py:841] 2021-02-10 20:24:20,980 >>   Num Epochs = 7\n",
            "[INFO|trainer.py:842] 2021-02-10 20:24:20,980 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:843] 2021-02-10 20:24:20,981 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:844] 2021-02-10 20:24:20,981 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:845] 2021-02-10 20:24:20,981 >>   Total optimization steps = 42\n",
            "[WARNING|training_args.py:514] 2021-02-10 20:24:20,987 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "\r  0%|          | 0/42 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "\r  2%|▏         | 1/42 [00:00<00:25,  1.60it/s]\r  5%|▍         | 2/42 [00:01<00:24,  1.60it/s]\r  7%|▋         | 3/42 [00:01<00:24,  1.57it/s]\r 10%|▉         | 4/42 [00:02<00:23,  1.59it/s]\r 12%|█▏        | 5/42 [00:03<00:23,  1.57it/s]\r 14%|█▍        | 6/42 [00:03<00:23,  1.55it/s]\r 17%|█▋        | 7/42 [00:04<00:22,  1.57it/s]\r 19%|█▉        | 8/42 [00:05<00:21,  1.55it/s]\r 21%|██▏       | 9/42 [00:05<00:21,  1.54it/s]\r 24%|██▍       | 10/42 [00:06<00:20,  1.54it/s]\r 26%|██▌       | 11/42 [00:07<00:20,  1.53it/s]\r 29%|██▊       | 12/42 [00:07<00:19,  1.53it/s]\r 31%|███       | 13/42 [00:08<00:19,  1.52it/s]\r 33%|███▎      | 14/42 [00:09<00:18,  1.52it/s]\r 36%|███▌      | 15/42 [00:09<00:17,  1.52it/s]\r 38%|███▊      | 16/42 [00:10<00:17,  1.52it/s]\r 40%|████      | 17/42 [00:11<00:16,  1.52it/s]\r 43%|████▎     | 18/42 [00:11<00:15,  1.52it/s]\r 45%|████▌     | 19/42 [00:12<00:15,  1.52it/s]\r 48%|████▊     | 20/42 [00:13<00:14,  1.52it/s]\r 50%|█████     | 21/42 [00:13<00:13,  1.52it/s]\r 52%|█████▏    | 22/42 [00:14<00:13,  1.52it/s]\r 55%|█████▍    | 23/42 [00:15<00:12,  1.52it/s]\r 57%|█████▋    | 24/42 [00:15<00:11,  1.51it/s]\r 60%|█████▉    | 25/42 [00:16<00:11,  1.51it/s]\r 62%|██████▏   | 26/42 [00:16<00:10,  1.51it/s]\r 64%|██████▍   | 27/42 [00:17<00:09,  1.51it/s]\r 67%|██████▋   | 28/42 [00:18<00:09,  1.52it/s]\r 69%|██████▉   | 29/42 [00:18<00:08,  1.52it/s]\r 71%|███████▏  | 30/42 [00:19<00:07,  1.51it/s]\r 74%|███████▍  | 31/42 [00:20<00:07,  1.51it/s]\r 76%|███████▌  | 32/42 [00:20<00:06,  1.51it/s]\r 79%|███████▊  | 33/42 [00:21<00:05,  1.51it/s]\r 81%|████████  | 34/42 [00:22<00:05,  1.51it/s]\r 83%|████████▎ | 35/42 [00:22<00:04,  1.51it/s]\r 86%|████████▌ | 36/42 [00:23<00:03,  1.52it/s]\r 88%|████████▊ | 37/42 [00:24<00:03,  1.52it/s]\r 90%|█████████ | 38/42 [00:24<00:02,  1.52it/s]\r 93%|█████████▎| 39/42 [00:25<00:01,  1.52it/s]\r 95%|█████████▌| 40/42 [00:26<00:01,  1.52it/s]\r 98%|█████████▊| 41/42 [00:26<00:00,  1.52it/s]\r100%|██████████| 42/42 [00:27<00:00,  1.52it/s][INFO|trainer.py:1013] 2021-02-10 20:24:48,550 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "\r                                               \r\r100%|██████████| 42/42 [00:27<00:00,  1.52it/s]\r100%|██████████| 42/42 [00:27<00:00,  1.52it/s]\n",
            "[INFO|trainer.py:1422] 2021-02-10 20:24:48,553 >> Saving model checkpoint to /content/drive/MyDrive/NLP/billsum/output7\n",
            "[INFO|configuration_utils.py:311] 2021-02-10 20:24:48,563 >> Configuration saved in /content/drive/MyDrive/NLP/billsum/output7/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-02-10 20:24:56,004 >> Model weights saved in /content/drive/MyDrive/NLP/billsum/output7/pytorch_model.bin\n",
            "[INFO|trainer.py:1620] 2021-02-10 20:24:58,948 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1621] 2021-02-10 20:24:58,948 >>   Num examples = 7\n",
            "[INFO|trainer.py:1622] 2021-02-10 20:24:58,949 >>   Batch size = 8\n",
            "\r  0%|          | 0/1 [00:00<?, ?it/s]\r100%|██████████| 1/1 [00:00<00:00,  9.88it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOUUt8pP8hhr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31d04320-2a4b-444a-9736-a8bb209938fd"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/      run_clm.py.1  run_clm.py.3  \u001b[01;34mruns\u001b[0m/         \u001b[01;34msession\u001b[0m/\n",
            "run_clm.py  run_clm.py.2  run_clm.py.4  \u001b[01;34msample_data\u001b[0m/  \u001b[01;34mtransformers\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN40lwEa9VA7"
      },
      "source": [
        "# Generating text below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHYpC6FOuPHo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e48d2fb3-2d5e-4012-d7c8-d3a6f29a796f"
      },
      "source": [
        "%%bash\n",
        "cd transformers/examples/text-generation\n",
        "CUDA_VISIBLE_DEVICES=$N python run_generation.py \\\n",
        "--model_type gpt2 \\\n",
        "--model_name_or_path /content/drive/MyDrive/NLP/billsum/output7 \\\n",
        "--length 50 \\\n",
        "--prompt \"<BOS> I was late in seeing\" \\\n",
        "--stop_token \"<EOS>\" \\\n",
        "--k 5 \\\n",
        "--num_return_sequences 8"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== GENERATED SEQUENCE 1 ===\n",
            "<BOS> I was late in seeing my wife. I'll see you later. <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> \n",
            "=== GENERATED SEQUENCE 2 ===\n",
            "<BOS> I was late in seeing the movie and it was a great film, great cast. I loved it. I don't know if it was the location, the weather, or whatever, but I was a little bit late and I didn't get to see it.\n",
            "\n",
            "=== GENERATED SEQUENCE 3 ===\n",
            "<BOS> I was late in seeing you, and you were a good sport. I was glad to hear that you're not a big fan of the game, and that you're not going to be a fan of any team that drafts you. I know you're a great person\n",
            "=== GENERATED SEQUENCE 4 ===\n",
            "<BOS> I was late in seeing the movie. <BOS> So I don't really know. <BOS> But I know you are a big fan and I am glad that you are seeing the movie. <BOS> Thank you so much for taking the time t\n",
            "=== GENERATED SEQUENCE 5 ===\n",
            "<BOS> I was late in seeing the game and I don't want to be late again, but I'll be there if you are. <BOS> You are welcome <BOS> <BOS> I'll be there <BOS>\n",
            "\n",
            "<BOS\n",
            "=== GENERATED SEQUENCE 6 ===\n",
            "<BOS> I was late in seeing you. You were late for a meeting at the hospital with your father. [A6A6I5] ====> 20/06/15 dialoglog BOS: I'm sorry about that. You were a good kid. I know\n",
            "=== GENERATED SEQUENCE 7 ===\n",
            "<BOS> I was late in seeing the show, but I did catch it on Netflix and was glad I did. The show has a lot of humor and is well-paced. The writing is good and the actors do a great job with the characters. I would recommend this show t\n",
            "=== GENERATED SEQUENCE 8 ===\n",
            "<BOS> I was late in seeing you and I'm sorry. [BOS] I was going to say thank you for your time. But you're late, and I'm not sure if I'm even going to see you again. [COUNT] [Sigh] Yo\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/10/2021 20:25:03 - WARNING - __main__ -   device: cpu, n_gpu: 0, 16-bits training: False\n",
            "02/10/2021 20:25:20 - INFO - __main__ -   Namespace(device=device(type='cpu'), fp16=False, k=5, length=50, model_name_or_path='/content/drive/MyDrive/NLP/billsum/output7', model_type='gpt2', n_gpu=0, no_cuda=False, num_return_sequences=8, p=0.9, padding_text='', prefix='', prompt='<BOS> I was late in seeing', repetition_penalty=1.0, seed=42, stop_token='<EOS>', temperature=1.0, xlm_language='')\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "02/10/2021 20:25:20 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "2021-02-10 20:25:44.272865: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI2eR6DGwPzB"
      },
      "source": [
        "# Changing Learning rates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puQPwBX_jOLi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57903759-deb4-40d8-da99-e2e30ac413be"
      },
      "source": [
        "# 5e-1\n",
        "%%bash\n",
        "python run_clm.py \\\n",
        "--model_type gpt2 \\\n",
        "--model_name_or_path gpt2 \\\n",
        "--train_file /content/drive/MyDrive/NLP/billsum/d_train.txt \\\n",
        "--do_train \\\n",
        "--validation_file /content/drive/MyDrive/NLP/billsum/d_val.txt \\\n",
        "--do_eval \\\n",
        "--per_gpu_train_batch_size 1 \\\n",
        "--save_steps -1 \\\n",
        "--num_train_epochs 5 \\\n",
        "--fp16 \\\n",
        "--learning_rate 4e-1 \\\n",
        "--output_dir /content/drive/MyDrive/NLP/billsum/output5e1 \\\n",
        "--overwrite_output_dir \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/10/2021 20:25:50 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "02/10/2021 20:25:50 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/content/drive/MyDrive/NLP/billsum/output5e1, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.4, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Feb10_20-25-50_38bb79fdf945, logging_first_step=False, logging_steps=500, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/content/drive/MyDrive/NLP/billsum/output5e1, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, _n_gpu=1)\n",
            "02/10/2021 20:25:51 - INFO - transformers.configuration_utils -   loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "02/10/2021 20:25:51 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/10/2021 20:25:51 - INFO - transformers.configuration_utils -   loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "02/10/2021 20:25:51 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/10/2021 20:25:52 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "02/10/2021 20:25:52 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "02/10/2021 20:25:52 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "02/10/2021 20:25:52 - INFO - transformers.modeling_utils -   loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "02/10/2021 20:25:57 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "02/10/2021 20:25:57 - INFO - transformers.modeling_utils -   All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "02/10/2021 20:26:02 - INFO - transformers.trainer -   The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "02/10/2021 20:26:02 - INFO - transformers.trainer -   The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "02/10/2021 20:26:02 - INFO - transformers.trainer -   Using amp fp16 backend\n",
            "02/10/2021 20:26:02 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "02/10/2021 20:26:02 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "02/10/2021 20:26:02 - INFO - transformers.trainer -   ***** Running training *****\n",
            "02/10/2021 20:26:02 - INFO - transformers.trainer -     Num examples = 6\n",
            "02/10/2021 20:26:02 - INFO - transformers.trainer -     Num Epochs = 5\n",
            "02/10/2021 20:26:02 - INFO - transformers.trainer -     Instantaneous batch size per device = 8\n",
            "02/10/2021 20:26:02 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "02/10/2021 20:26:02 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
            "02/10/2021 20:26:02 - INFO - transformers.trainer -     Total optimization steps = 30\n",
            "02/10/2021 20:26:02 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "02/10/2021 20:26:08 - INFO - transformers.trainer -   \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 6.9372, 'train_samples_per_second': 4.324, 'epoch': 5.0}\n",
            "02/10/2021 20:26:08 - INFO - transformers.trainer -   Saving model checkpoint to /content/drive/MyDrive/NLP/billsum/output5e1\n",
            "02/10/2021 20:26:09 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/MyDrive/NLP/billsum/output5e1/config.json\n",
            "02/10/2021 20:26:11 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/MyDrive/NLP/billsum/output5e1/pytorch_model.bin\n",
            "02/10/2021 20:26:11 - INFO - __main__ -   ***** Train results *****\n",
            "02/10/2021 20:26:11 - INFO - __main__ -     epoch = 5.0\n",
            "02/10/2021 20:26:11 - INFO - __main__ -     train_runtime = 6.9372\n",
            "02/10/2021 20:26:11 - INFO - __main__ -     train_samples_per_second = 4.324\n",
            "02/10/2021 20:26:11 - INFO - __main__ -   *** Evaluate ***\n",
            "02/10/2021 20:26:11 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
            "02/10/2021 20:26:11 - INFO - transformers.trainer -     Num examples = 7\n",
            "02/10/2021 20:26:11 - INFO - transformers.trainer -     Batch size = 8\n",
            "02/10/2021 20:26:12 - INFO - __main__ -   ***** Eval results *****\n",
            "02/10/2021 20:26:12 - INFO - __main__ -     eval_loss = 194.28916931152344\n",
            "02/10/2021 20:26:12 - INFO - __main__ -     perplexity = 2.3917408741696233e+84\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-02-10 20:25:49.033332: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "Using custom data configuration default\n",
            "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-578db1fdc85307a7/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab)\n",
            "[INFO|configuration_utils.py:456] 2021-02-10 20:25:51,384 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:492] 2021-02-10 20:25:51,385 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:456] 2021-02-10 20:25:51,589 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:492] 2021-02-10 20:25:51,589 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 20:25:52,221 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 20:25:52,221 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 20:25:52,221 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|modeling_utils.py:1027] 2021-02-10 20:25:52,506 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1143] 2021-02-10 20:25:57,624 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1152] 2021-02-10 20:25:57,624 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-578db1fdc85307a7/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-ecc9266fbb545492.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-578db1fdc85307a7/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-49795fa8b8bd449b.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-578db1fdc85307a7/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-53f0e6cd35cfbccf.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-578db1fdc85307a7/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-6dc34314cfc55680.arrow\n",
            "[INFO|trainer.py:434] 2021-02-10 20:26:02,048 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:434] 2021-02-10 20:26:02,049 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:350] 2021-02-10 20:26:02,049 >> Using amp fp16 backend\n",
            "[WARNING|training_args.py:514] 2021-02-10 20:26:02,050 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:514] 2021-02-10 20:26:02,053 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:839] 2021-02-10 20:26:02,053 >> ***** Running training *****\n",
            "[INFO|trainer.py:840] 2021-02-10 20:26:02,053 >>   Num examples = 6\n",
            "[INFO|trainer.py:841] 2021-02-10 20:26:02,053 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:842] 2021-02-10 20:26:02,054 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:843] 2021-02-10 20:26:02,054 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:844] 2021-02-10 20:26:02,054 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:845] 2021-02-10 20:26:02,054 >>   Total optimization steps = 30\n",
            "[WARNING|training_args.py:514] 2021-02-10 20:26:02,058 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "\r  0%|          | 0/30 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "\r  3%|▎         | 1/30 [00:00<00:07,  4.00it/s]\r  7%|▋         | 2/30 [00:00<00:06,  4.09it/s]\r 10%|█         | 3/30 [00:00<00:06,  4.04it/s]\r 13%|█▎        | 4/30 [00:00<00:06,  4.12it/s]\r 17%|█▋        | 5/30 [00:01<00:05,  4.18it/s]\r 20%|██        | 6/30 [00:01<00:05,  4.14it/s]\r 23%|██▎       | 7/30 [00:01<00:05,  4.21it/s]\r 27%|██▋       | 8/30 [00:01<00:05,  4.25it/s]\r 30%|███       | 9/30 [00:02<00:04,  4.29it/s]\r 33%|███▎      | 10/30 [00:02<00:04,  4.32it/s]\r 37%|███▋      | 11/30 [00:02<00:04,  4.34it/s]\r 40%|████      | 12/30 [00:02<00:04,  4.36it/s]\r 43%|████▎     | 13/30 [00:03<00:03,  4.36it/s]\r 47%|████▋     | 14/30 [00:03<00:03,  4.38it/s]\r 50%|█████     | 15/30 [00:03<00:03,  4.38it/s]\r 53%|█████▎    | 16/30 [00:03<00:03,  4.35it/s]\r 57%|█████▋    | 17/30 [00:03<00:02,  4.36it/s]\r 60%|██████    | 18/30 [00:04<00:02,  4.37it/s]\r 63%|██████▎   | 19/30 [00:04<00:02,  4.38it/s]\r 67%|██████▋   | 20/30 [00:04<00:02,  4.38it/s]\r 70%|███████   | 21/30 [00:04<00:02,  4.38it/s]\r 73%|███████▎  | 22/30 [00:05<00:01,  4.38it/s]\r 77%|███████▋  | 23/30 [00:05<00:01,  4.38it/s]\r 80%|████████  | 24/30 [00:05<00:01,  4.38it/s]\r 83%|████████▎ | 25/30 [00:05<00:01,  4.38it/s]\r 87%|████████▋ | 26/30 [00:06<00:00,  4.38it/s]\r 90%|█████████ | 27/30 [00:06<00:00,  4.39it/s]\r 93%|█████████▎| 28/30 [00:06<00:00,  4.39it/s]\r 97%|█████████▋| 29/30 [00:06<00:00,  4.39it/s]\r100%|██████████| 30/30 [00:06<00:00,  4.36it/s][INFO|trainer.py:1013] 2021-02-10 20:26:08,991 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "\r                                               \r\r100%|██████████| 30/30 [00:06<00:00,  4.36it/s]\r100%|██████████| 30/30 [00:06<00:00,  4.33it/s]\n",
            "[INFO|trainer.py:1422] 2021-02-10 20:26:08,993 >> Saving model checkpoint to /content/drive/MyDrive/NLP/billsum/output5e1\n",
            "[INFO|configuration_utils.py:311] 2021-02-10 20:26:09,003 >> Configuration saved in /content/drive/MyDrive/NLP/billsum/output5e1/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-02-10 20:26:11,552 >> Model weights saved in /content/drive/MyDrive/NLP/billsum/output5e1/pytorch_model.bin\n",
            "[INFO|trainer.py:1620] 2021-02-10 20:26:11,676 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1621] 2021-02-10 20:26:11,676 >>   Num examples = 7\n",
            "[INFO|trainer.py:1622] 2021-02-10 20:26:11,677 >>   Batch size = 8\n",
            "\r  0%|          | 0/1 [00:00<?, ?it/s]\r100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyhZ5F3vY7BD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb215a3a-0f41-4266-9ee5-2033432a5537"
      },
      "source": [
        "# 4e-2\n",
        "%%bash\n",
        "python run_clm.py \\\n",
        "--model_type gpt2 \\\n",
        "--model_name_or_path gpt2 \\\n",
        "--train_file /content/drive/MyDrive/NLP/billsum/d_train.txt \\\n",
        "--do_train \\\n",
        "--validation_file /content/drive/MyDrive/NLP/billsum/d_val.txt \\\n",
        "--do_eval \\\n",
        "--per_gpu_train_batch_size 1 \\\n",
        "--save_steps -1 \\\n",
        "--num_train_epochs 5 \\\n",
        "--fp16 \\\n",
        "--learning_rate 4e-2 \\\n",
        "--output_dir /content/drive/MyDrive/NLP/billsum/output5e2 \\\n",
        "--overwrite_output_dir \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/10/2021 20:26:16 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "02/10/2021 20:26:16 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/content/drive/MyDrive/NLP/billsum/output5e2, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.04, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Feb10_20-26-16_38bb79fdf945, logging_first_step=False, logging_steps=500, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/content/drive/MyDrive/NLP/billsum/output5e2, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, _n_gpu=1)\n",
            "02/10/2021 20:26:17 - INFO - transformers.configuration_utils -   loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "02/10/2021 20:26:17 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/10/2021 20:26:17 - INFO - transformers.configuration_utils -   loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "02/10/2021 20:26:17 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/10/2021 20:26:17 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "02/10/2021 20:26:17 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "02/10/2021 20:26:17 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "02/10/2021 20:26:18 - INFO - transformers.modeling_utils -   loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "02/10/2021 20:26:23 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "02/10/2021 20:26:23 - INFO - transformers.modeling_utils -   All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "02/10/2021 20:26:28 - INFO - transformers.trainer -   The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "02/10/2021 20:26:28 - INFO - transformers.trainer -   The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "02/10/2021 20:26:28 - INFO - transformers.trainer -   Using amp fp16 backend\n",
            "02/10/2021 20:26:28 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "02/10/2021 20:26:28 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "02/10/2021 20:26:28 - INFO - transformers.trainer -   ***** Running training *****\n",
            "02/10/2021 20:26:28 - INFO - transformers.trainer -     Num examples = 6\n",
            "02/10/2021 20:26:28 - INFO - transformers.trainer -     Num Epochs = 5\n",
            "02/10/2021 20:26:28 - INFO - transformers.trainer -     Instantaneous batch size per device = 8\n",
            "02/10/2021 20:26:28 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "02/10/2021 20:26:28 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
            "02/10/2021 20:26:28 - INFO - transformers.trainer -     Total optimization steps = 30\n",
            "02/10/2021 20:26:28 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "02/10/2021 20:26:35 - INFO - transformers.trainer -   \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 7.2786, 'train_samples_per_second': 4.122, 'epoch': 5.0}\n",
            "02/10/2021 20:26:35 - INFO - transformers.trainer -   Saving model checkpoint to /content/drive/MyDrive/NLP/billsum/output5e2\n",
            "02/10/2021 20:26:35 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/MyDrive/NLP/billsum/output5e2/config.json\n",
            "02/10/2021 20:26:38 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/MyDrive/NLP/billsum/output5e2/pytorch_model.bin\n",
            "02/10/2021 20:26:38 - INFO - __main__ -   ***** Train results *****\n",
            "02/10/2021 20:26:38 - INFO - __main__ -     epoch = 5.0\n",
            "02/10/2021 20:26:38 - INFO - __main__ -     train_runtime = 7.2786\n",
            "02/10/2021 20:26:38 - INFO - __main__ -     train_samples_per_second = 4.122\n",
            "02/10/2021 20:26:38 - INFO - __main__ -   *** Evaluate ***\n",
            "02/10/2021 20:26:38 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
            "02/10/2021 20:26:38 - INFO - transformers.trainer -     Num examples = 7\n",
            "02/10/2021 20:26:38 - INFO - transformers.trainer -     Batch size = 8\n",
            "02/10/2021 20:26:39 - INFO - __main__ -   ***** Eval results *****\n",
            "02/10/2021 20:26:39 - INFO - __main__ -     eval_loss = 48.799434661865234\n",
            "02/10/2021 20:26:39 - INFO - __main__ -     perplexity = 1.5607207112880106e+21\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-02-10 20:26:15.016289: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "Using custom data configuration default\n",
            "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-578db1fdc85307a7/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab)\n",
            "[INFO|configuration_utils.py:456] 2021-02-10 20:26:17,062 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:492] 2021-02-10 20:26:17,063 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:456] 2021-02-10 20:26:17,267 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:492] 2021-02-10 20:26:17,268 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 20:26:17,915 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 20:26:17,915 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 20:26:17,915 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|modeling_utils.py:1027] 2021-02-10 20:26:18,189 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1143] 2021-02-10 20:26:23,769 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1152] 2021-02-10 20:26:23,769 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-578db1fdc85307a7/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-ecc9266fbb545492.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-578db1fdc85307a7/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-49795fa8b8bd449b.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-578db1fdc85307a7/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-53f0e6cd35cfbccf.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-578db1fdc85307a7/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-6dc34314cfc55680.arrow\n",
            "[INFO|trainer.py:434] 2021-02-10 20:26:28,534 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:434] 2021-02-10 20:26:28,534 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:350] 2021-02-10 20:26:28,535 >> Using amp fp16 backend\n",
            "[WARNING|training_args.py:514] 2021-02-10 20:26:28,536 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:514] 2021-02-10 20:26:28,539 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:839] 2021-02-10 20:26:28,539 >> ***** Running training *****\n",
            "[INFO|trainer.py:840] 2021-02-10 20:26:28,539 >>   Num examples = 6\n",
            "[INFO|trainer.py:841] 2021-02-10 20:26:28,539 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:842] 2021-02-10 20:26:28,539 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:843] 2021-02-10 20:26:28,539 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:844] 2021-02-10 20:26:28,539 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:845] 2021-02-10 20:26:28,539 >>   Total optimization steps = 30\n",
            "[WARNING|training_args.py:514] 2021-02-10 20:26:28,544 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "\r  0%|          | 0/30 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "\r  3%|▎         | 1/30 [00:00<00:07,  4.11it/s]\r  7%|▋         | 2/30 [00:00<00:06,  4.17it/s]\r 10%|█         | 3/30 [00:00<00:06,  4.08it/s]\r 13%|█▎        | 4/30 [00:00<00:06,  4.17it/s]\r 17%|█▋        | 5/30 [00:01<00:05,  4.24it/s]\r 20%|██        | 6/30 [00:01<00:05,  4.26it/s]\r 23%|██▎       | 7/30 [00:01<00:05,  4.20it/s]\r 27%|██▋       | 8/30 [00:01<00:05,  4.17it/s]\r 30%|███       | 9/30 [00:02<00:05,  4.14it/s]\r 33%|███▎      | 10/30 [00:02<00:04,  4.13it/s]\r 37%|███▋      | 11/30 [00:02<00:04,  4.11it/s]\r 40%|████      | 12/30 [00:02<00:04,  4.10it/s]\r 43%|████▎     | 13/30 [00:03<00:04,  4.09it/s]\r 47%|████▋     | 14/30 [00:03<00:03,  4.09it/s]\r 50%|█████     | 15/30 [00:03<00:03,  4.09it/s]\r 53%|█████▎    | 16/30 [00:03<00:03,  4.09it/s]\r 57%|█████▋    | 17/30 [00:04<00:03,  4.09it/s]\r 60%|██████    | 18/30 [00:04<00:02,  4.08it/s]\r 63%|██████▎   | 19/30 [00:04<00:02,  4.08it/s]\r 67%|██████▋   | 20/30 [00:04<00:02,  4.08it/s]\r 70%|███████   | 21/30 [00:05<00:02,  4.08it/s]\r 73%|███████▎  | 22/30 [00:05<00:01,  4.08it/s]\r 77%|███████▋  | 23/30 [00:05<00:01,  4.08it/s]\r 80%|████████  | 24/30 [00:05<00:01,  4.08it/s]\r 83%|████████▎ | 25/30 [00:06<00:01,  4.07it/s]\r 87%|████████▋ | 26/30 [00:06<00:00,  4.08it/s]\r 90%|█████████ | 27/30 [00:06<00:00,  4.09it/s]\r 93%|█████████▎| 28/30 [00:06<00:00,  4.17it/s]\r 97%|█████████▋| 29/30 [00:07<00:00,  4.22it/s]\r100%|██████████| 30/30 [00:07<00:00,  4.18it/s][INFO|trainer.py:1013] 2021-02-10 20:26:35,818 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "\r                                               \r\r100%|██████████| 30/30 [00:07<00:00,  4.18it/s]\r100%|██████████| 30/30 [00:07<00:00,  4.13it/s]\n",
            "[INFO|trainer.py:1422] 2021-02-10 20:26:35,820 >> Saving model checkpoint to /content/drive/MyDrive/NLP/billsum/output5e2\n",
            "[INFO|configuration_utils.py:311] 2021-02-10 20:26:35,826 >> Configuration saved in /content/drive/MyDrive/NLP/billsum/output5e2/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-02-10 20:26:38,679 >> Model weights saved in /content/drive/MyDrive/NLP/billsum/output5e2/pytorch_model.bin\n",
            "[INFO|trainer.py:1620] 2021-02-10 20:26:38,792 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1621] 2021-02-10 20:26:38,792 >>   Num examples = 7\n",
            "[INFO|trainer.py:1622] 2021-02-10 20:26:38,793 >>   Batch size = 8\n",
            "\r  0%|          | 0/1 [00:00<?, ?it/s]\r100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cluuncuZIqA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f5ea90e-41f0-4bbe-ee0e-513d7e0241a8"
      },
      "source": [
        "# 3e-3\n",
        "%%bash\n",
        "python run_clm.py \\\n",
        "--model_type gpt2 \\\n",
        "--model_name_or_path gpt2 \\\n",
        "--train_file /content/drive/MyDrive/NLP/billsum/d_train.txt \\\n",
        "--do_train \\\n",
        "--validation_file /content/drive/MyDrive/NLP/billsum/d_val.txt \\\n",
        "--do_eval \\\n",
        "--per_gpu_train_batch_size 1 \\\n",
        "--save_steps -1 \\\n",
        "--num_train_epochs 5 \\\n",
        "--fp16 \\\n",
        "--learning_rate 4e-3 \\\n",
        "--output_dir /content/drive/MyDrive/NLP/billsum/output5e3 \\\n",
        "--overwrite_output_dir \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/10/2021 20:26:43 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "02/10/2021 20:26:43 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/content/drive/MyDrive/NLP/billsum/output5e3, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.004, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Feb10_20-26-43_38bb79fdf945, logging_first_step=False, logging_steps=500, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/content/drive/MyDrive/NLP/billsum/output5e3, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, _n_gpu=1)\n",
            "02/10/2021 20:26:44 - INFO - transformers.configuration_utils -   loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "02/10/2021 20:26:44 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/10/2021 20:26:44 - INFO - transformers.configuration_utils -   loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "02/10/2021 20:26:44 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/10/2021 20:26:44 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "02/10/2021 20:26:44 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "02/10/2021 20:26:44 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "02/10/2021 20:26:45 - INFO - transformers.modeling_utils -   loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "02/10/2021 20:26:50 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "02/10/2021 20:26:50 - INFO - transformers.modeling_utils -   All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "02/10/2021 20:26:55 - INFO - transformers.trainer -   The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "02/10/2021 20:26:55 - INFO - transformers.trainer -   The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "02/10/2021 20:26:55 - INFO - transformers.trainer -   Using amp fp16 backend\n",
            "02/10/2021 20:26:55 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "02/10/2021 20:26:55 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "02/10/2021 20:26:55 - INFO - transformers.trainer -   ***** Running training *****\n",
            "02/10/2021 20:26:55 - INFO - transformers.trainer -     Num examples = 6\n",
            "02/10/2021 20:26:55 - INFO - transformers.trainer -     Num Epochs = 5\n",
            "02/10/2021 20:26:55 - INFO - transformers.trainer -     Instantaneous batch size per device = 8\n",
            "02/10/2021 20:26:55 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "02/10/2021 20:26:55 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
            "02/10/2021 20:26:55 - INFO - transformers.trainer -     Total optimization steps = 30\n",
            "02/10/2021 20:26:55 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "02/10/2021 20:27:02 - INFO - transformers.trainer -   \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 7.2999, 'train_samples_per_second': 4.11, 'epoch': 5.0}\n",
            "02/10/2021 20:27:02 - INFO - transformers.trainer -   Saving model checkpoint to /content/drive/MyDrive/NLP/billsum/output5e3\n",
            "02/10/2021 20:27:02 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/MyDrive/NLP/billsum/output5e3/config.json\n",
            "02/10/2021 20:27:05 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/MyDrive/NLP/billsum/output5e3/pytorch_model.bin\n",
            "02/10/2021 20:27:05 - INFO - __main__ -   ***** Train results *****\n",
            "02/10/2021 20:27:05 - INFO - __main__ -     epoch = 5.0\n",
            "02/10/2021 20:27:05 - INFO - __main__ -     train_runtime = 7.2999\n",
            "02/10/2021 20:27:05 - INFO - __main__ -     train_samples_per_second = 4.11\n",
            "02/10/2021 20:27:05 - INFO - __main__ -   *** Evaluate ***\n",
            "02/10/2021 20:27:05 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
            "02/10/2021 20:27:05 - INFO - transformers.trainer -     Num examples = 7\n",
            "02/10/2021 20:27:05 - INFO - transformers.trainer -     Batch size = 8\n",
            "02/10/2021 20:27:06 - INFO - __main__ -   ***** Eval results *****\n",
            "02/10/2021 20:27:06 - INFO - __main__ -     eval_loss = 9.069488525390625\n",
            "02/10/2021 20:27:06 - INFO - __main__ -     perplexity = 8686.179908864977\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-02-10 20:26:42.141904: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "Using custom data configuration default\n",
            "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-578db1fdc85307a7/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab)\n",
            "[INFO|configuration_utils.py:456] 2021-02-10 20:26:44,084 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:492] 2021-02-10 20:26:44,085 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:456] 2021-02-10 20:26:44,289 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:492] 2021-02-10 20:26:44,289 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 20:26:44,931 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 20:26:44,931 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-10 20:26:44,931 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|modeling_utils.py:1027] 2021-02-10 20:26:45,213 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1143] 2021-02-10 20:26:50,966 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1152] 2021-02-10 20:26:50,966 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-578db1fdc85307a7/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-ecc9266fbb545492.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-578db1fdc85307a7/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-49795fa8b8bd449b.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-578db1fdc85307a7/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-53f0e6cd35cfbccf.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-578db1fdc85307a7/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-6dc34314cfc55680.arrow\n",
            "[INFO|trainer.py:434] 2021-02-10 20:26:55,488 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:434] 2021-02-10 20:26:55,489 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:350] 2021-02-10 20:26:55,489 >> Using amp fp16 backend\n",
            "[WARNING|training_args.py:514] 2021-02-10 20:26:55,490 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:514] 2021-02-10 20:26:55,493 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:839] 2021-02-10 20:26:55,493 >> ***** Running training *****\n",
            "[INFO|trainer.py:840] 2021-02-10 20:26:55,493 >>   Num examples = 6\n",
            "[INFO|trainer.py:841] 2021-02-10 20:26:55,493 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:842] 2021-02-10 20:26:55,493 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:843] 2021-02-10 20:26:55,493 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:844] 2021-02-10 20:26:55,493 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:845] 2021-02-10 20:26:55,493 >>   Total optimization steps = 30\n",
            "[WARNING|training_args.py:514] 2021-02-10 20:26:55,498 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "\r  0%|          | 0/30 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "\r  3%|▎         | 1/30 [00:00<00:06,  4.27it/s]\r  7%|▋         | 2/30 [00:00<00:06,  4.29it/s]\r 10%|█         | 3/30 [00:00<00:06,  4.19it/s]\r 13%|█▎        | 4/30 [00:00<00:06,  4.23it/s]\r 17%|█▋        | 5/30 [00:01<00:05,  4.24it/s]\r 20%|██        | 6/30 [00:01<00:05,  4.26it/s]\r 23%|██▎       | 7/30 [00:01<00:05,  4.28it/s]\r 27%|██▋       | 8/30 [00:01<00:05,  4.22it/s]\r 30%|███       | 9/30 [00:02<00:05,  4.17it/s]\r 33%|███▎      | 10/30 [00:02<00:04,  4.14it/s]\r 37%|███▋      | 11/30 [00:02<00:04,  4.12it/s]\r 40%|████      | 12/30 [00:02<00:04,  4.11it/s]\r 43%|████▎     | 13/30 [00:03<00:04,  4.09it/s]\r 47%|████▋     | 14/30 [00:03<00:03,  4.09it/s]\r 50%|█████     | 15/30 [00:03<00:03,  4.09it/s]\r 53%|█████▎    | 16/30 [00:03<00:03,  4.09it/s]\r 57%|█████▋    | 17/30 [00:04<00:03,  4.08it/s]\r 60%|██████    | 18/30 [00:04<00:02,  4.08it/s]\r 63%|██████▎   | 19/30 [00:04<00:02,  4.08it/s]\r 67%|██████▋   | 20/30 [00:04<00:02,  4.08it/s]\r 70%|███████   | 21/30 [00:05<00:02,  4.07it/s]\r 73%|███████▎  | 22/30 [00:05<00:01,  4.08it/s]\r 77%|███████▋  | 23/30 [00:05<00:01,  4.08it/s]\r 80%|████████  | 24/30 [00:05<00:01,  4.08it/s]\r 83%|████████▎ | 25/30 [00:06<00:01,  4.08it/s]\r 87%|████████▋ | 26/30 [00:06<00:00,  4.09it/s]\r 90%|█████████ | 27/30 [00:06<00:00,  4.08it/s]\r 93%|█████████▎| 28/30 [00:06<00:00,  4.09it/s]\r 97%|█████████▋| 29/30 [00:07<00:00,  4.09it/s]\r100%|██████████| 30/30 [00:07<00:00,  4.05it/s][INFO|trainer.py:1013] 2021-02-10 20:27:02,793 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "\r                                               \r\r100%|██████████| 30/30 [00:07<00:00,  4.05it/s]\r100%|██████████| 30/30 [00:07<00:00,  4.12it/s]\n",
            "[INFO|trainer.py:1422] 2021-02-10 20:27:02,795 >> Saving model checkpoint to /content/drive/MyDrive/NLP/billsum/output5e3\n",
            "[INFO|configuration_utils.py:311] 2021-02-10 20:27:02,801 >> Configuration saved in /content/drive/MyDrive/NLP/billsum/output5e3/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-02-10 20:27:05,550 >> Model weights saved in /content/drive/MyDrive/NLP/billsum/output5e3/pytorch_model.bin\n",
            "[INFO|trainer.py:1620] 2021-02-10 20:27:05,997 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1621] 2021-02-10 20:27:05,997 >>   Num examples = 7\n",
            "[INFO|trainer.py:1622] 2021-02-10 20:27:05,998 >>   Batch size = 8\n",
            "\r  0%|          | 0/1 [00:00<?, ?it/s]\r100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57mD1G-FG4_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74dcbbec-e0fd-45b0-de7e-eb3fbcc5b1bb"
      },
      "source": [
        "%%bash\r\n",
        "cd transformers/examples/text-generation\r\n",
        "CUDA_VISIBLE_DEVICES=$N python run_generation.py \\\r\n",
        "--model_type gpt2 \\\r\n",
        "--model_name_or_path /content/drive/MyDrive/NLP/billsum/output7 \\\r\n",
        "--length 50 \\\r\n",
        "--prompt \"<BOS> I was late in seeing\" \\\r\n",
        "--stop_token \"<EOS>\" \\\r\n",
        "--k 5 \\\r\n",
        "--num_return_sequences 8\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== GENERATED SEQUENCE 1 ===\n",
            "<BOS> I was late in seeing my wife. I'll see you later. <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> \n",
            "=== GENERATED SEQUENCE 2 ===\n",
            "<BOS> I was late in seeing the movie and it was a great film, great cast. I loved it. I don't know if it was the location, the weather, or whatever, but I was a little bit late and I didn't get to see it.\n",
            "\n",
            "=== GENERATED SEQUENCE 3 ===\n",
            "<BOS> I was late in seeing you, and you were a good sport. I was glad to hear that you're not a big fan of the game, and that you're not going to be a fan of any team that drafts you. I know you're a great person\n",
            "=== GENERATED SEQUENCE 4 ===\n",
            "<BOS> I was late in seeing the movie. <BOS> So I don't really know. <BOS> But I know you are a big fan and I am glad that you are seeing the movie. <BOS> Thank you so much for taking the time t\n",
            "=== GENERATED SEQUENCE 5 ===\n",
            "<BOS> I was late in seeing the game and I don't want to be late again, but I'll be there if you are. <BOS> You are welcome <BOS> <BOS> I'll be there <BOS>\n",
            "\n",
            "<BOS\n",
            "=== GENERATED SEQUENCE 6 ===\n",
            "<BOS> I was late in seeing you. You were late for a meeting at the hospital with your father. [A6A6I5] ====> 20/06/15 dialoglog BOS: I'm sorry about that. You were a good kid. I know\n",
            "=== GENERATED SEQUENCE 7 ===\n",
            "<BOS> I was late in seeing the show, but I did catch it on Netflix and was glad I did. The show has a lot of humor and is well-paced. The writing is good and the actors do a great job with the characters. I would recommend this show t\n",
            "=== GENERATED SEQUENCE 8 ===\n",
            "<BOS> I was late in seeing you and I'm sorry. [BOS] I was going to say thank you for your time. But you're late, and I'm not sure if I'm even going to see you again. [COUNT] [Sigh] Yo\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/10/2021 20:27:08 - WARNING - __main__ -   device: cpu, n_gpu: 0, 16-bits training: False\n",
            "02/10/2021 20:27:24 - INFO - __main__ -   Namespace(device=device(type='cpu'), fp16=False, k=5, length=50, model_name_or_path='/content/drive/MyDrive/NLP/billsum/output7', model_type='gpt2', n_gpu=0, no_cuda=False, num_return_sequences=8, p=0.9, padding_text='', prefix='', prompt='<BOS> I was late in seeing', repetition_penalty=1.0, seed=42, stop_token='<EOS>', temperature=1.0, xlm_language='')\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "02/10/2021 20:27:24 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "2021-02-10 20:27:47.801710: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvEEeNLBHJxQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}